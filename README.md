# Domain-AI Copilot: An Advanced RAG Chatbot

Domain-AI Copilot is a sophisticated, Retrieval-Augmented Generation (RAG) chatbot built with Streamlit. It allows you to create a powerful question-answering system based on your own documents. Simply upload your knowledge base (PDFs, DOCX, TXT files), and the AI Copilot will use it to provide accurate, context-aware, and cited answers.
<img width="1974" height="3248" alt="Mermaid Chart - Create complex, visual diagrams with text -2025-10-18-081500" src="https://github.com/user-attachments/assets/402bfe79-63c1-40de-b484-5317546392d8" />
## âœ¨ Key Features

-   **ğŸ“š Custom Knowledge Base**: Upload your own PDF, DOCX, and TXT files to build a private knowledge base for the chatbot to use.
-   **ğŸ§  Advanced RAG Pipeline**: This isn't a simple RAG. The system uses a multi-step process for high-quality answers:
    -   **History-Aware Queries**: The chatbot understands conversation context, rewriting follow-up questions (e.g., "why?") into standalone queries for better retrieval.
    -   **State-of-the-Art Embeddings**: Utilizes Cohere's powerful `embed-v4.0` models for accurate semantic understanding of your text.
    -   **Intelligent Re-ranking**: After initial retrieval from the vector store, it uses Cohere's Re-rank API to significantly improve the relevance of the documents passed to the language model.
    -   **Context-Aware Generation**: The final answer is generated by a powerful LLM (via OpenRouter) using only the most relevant, re-ranked context.
-   **ğŸ“– Source Citations**: Every answer generated from your documents is accompanied by expandable source citations, so you can verify the information and see the original context.
-   **ğŸ’¡ Follow-up Suggestions**: After each answer, the AI suggests relevant next questions to help guide the conversation and explore topics more deeply.
-   **âš¡ Caching for Performance**: Implements caching for embeddings and LLM calls to speed up repeated queries and reduce API costs.
-   **â˜ï¸ Fallback Mechanism**: If no relevant information is found in the uploaded documents, the chatbot gracefully falls back to its general knowledge to answer the question.
-   **âš™ï¸ Deduplication**: The data loader creates deterministic UUIDs for each text chunk, preventing duplicate content from being added to the knowledge base.

## âš™ï¸ How It Works: The RAG Pipeline

The application follows a sophisticated pipeline to answer user queries:

1.  **Data Ingestion (Sidebar)**:
    -   A user uploads files via the Streamlit sidebar.
    -   `data_loader.py` processes each file, parsing its content.
    -   The text is split into smaller, overlapping chunks using `tiktoken` for semantic consistency.
    -   Each chunk is assigned a unique, content-based UUID to prevent duplicates.
    -   New chunks are converted into vector embeddings using Cohere's API.
    -   The embeddings and their corresponding text are stored in a Qdrant vector database.

2.  **Query Processing (Main Chat)**:
    -   **Step 1: Query Rewriting**: The user's query and the last few turns of conversation history are sent to a fast LLM (`Mistral-7B`) to create a self-contained, "standalone" query.
    -   **Step 2: Embedding**: The standalone query is converted into a vector embedding.
    -   **Step 3: Retrieval**: The query embedding is used to search the Qdrant database for the `top_k` (e.g., 10) most similar document chunks.
    -   **Step 4: Re-ranking**: The initial `top_k` results and the query are sent to the Cohere Re-rank API, which re-orders them by relevance. The `top_n` (e.g., 5) most relevant documents are selected.
    -   **Step 5: Generation**: The re-ranked documents (the "context") and the standalone query are formatted into a final prompt. This prompt is sent to a powerful LLM (e.g., `DeepSeek Chat`) via OpenRouter to generate the final, human-readable answer.
    -   **Step 6: Display**: The answer is displayed in the chat interface, along with the source documents used to generate it and suggested follow-up questions.

## ğŸ› ï¸ Technology Stack

-   **Frontend**: [Streamlit](https://streamlit.io/)
-   **Backend**: Python
-   **Vector Database**: [Qdrant](https://qdrant.tech/)
-   **AI Services & Models**:
    -   **Embeddings & Re-ranking**: [Cohere](https://cohere.ai/) (using `rerank-multilingual-v3.0` and `embed-v4.0`)
    -   **LLM Serving**: [OpenRouter.ai](https://openrouter.ai/) (for models like `DeepSeek`, `Mistral`, etc.)
-   **Key Libraries**: `qdrant-client`, `cohere`, `openai` (for API compatibility), `pypdf`, `python-docx`, `tiktoken`, `python-dotenv`.

## ğŸš€ Setup and Installation

### 1. Prerequisites

-   Python 3.9+ and `pip`
-   Git

### 2. Clone the Repository

```
git clone https://github.com/nam-htran/DomainAIAgent
cd DomainAIAgent
```
3. Set up a Virtual Environment (Recommended)
#### For Unix/macOS
```
python3 -m venv venv
source venv/bin/activate
```

#### For Windows
```
python -m venv venv
.\venv\Scripts\activate
```
## âš™ï¸ Installation & Setup Guide

### 1. Install Dependencies
Create a `requirements.txt` file with the following content:

```
# requirements.txt
streamlit
qdrant-client
cohere
openai
python-dotenv
pypdf
python-docx
tiktoken
```
Then install them:

```
pip install -r requirements.txt
```
### 2. Configure Environment Variables
You need API keys for Qdrant, Cohere, and OpenRouter.

Create a .env file in the root directory and add the following lines (replace the placeholders):
```
# .env

# Qdrant
QDRANT_HOST="your-qdrant-cluster-url"
QDRANT_API_KEY="your-qdrant-api-key"

# Cohere
COHERE_API_KEY="your-cohere-api-key"

# OpenRouter
OPENROUTER_API_KEY="your-openrouter-api-key"

# RAG Config
COLLECTION_NAME="my-rag-collection"
EMBEDDING_DIM=1024     # embed-v4.0-light â†’ 1024 dims
LLM_MODEL="deepseek/deepseek-chat"
```
### 3. Run the Application
Once your .env and dependencies are ready, launch the Streamlit app:
```
streamlit run app.py
```
Your browser will automatically open the UI.

### 4. Usage Guide
Upload Documents â€“ Use the sidebar uploader to select one or more PDF, DOCX, or TXT files.

Process Files â€“ Click â€œProcess and Loadâ€ and wait for confirmation.

Ask Questions â€“ Type your question in the chat input.

View Answer â€“ The AI shows an answer, related sources, and follow-up suggestions.

### 5. Project Structure
```
/
â”œâ”€â”€ .cache/              # Cached embeddings & responses
â”œâ”€â”€ rag_engine/          # Core RAG modules
â”‚   â”œâ”€â”€ data_loader.py   # File ingestion & DB loading
â”‚   â”œâ”€â”€ embedding.py     # Embedding (Cohere API)
â”‚   â”œâ”€â”€ file_processor.py# File parsing
â”‚   â”œâ”€â”€ llm.py           # LLM query handling (OpenRouter)
â”‚   â”œâ”€â”€ reranker.py      # Document reranking
â”‚   â””â”€â”€ vector_store.py  # Qdrant DB operations
â”œâ”€â”€ app.py               # Streamlit main UI
â”œâ”€â”€ test.py              # CLI testing script
â”œâ”€â”€ .env                 # Environment variables
â””â”€â”€ requirements.txt     # Dependencies list
```
