
===== ./test.py =====
# test_rag.py

from rag_engine.embedding import get_embedding
from rag_engine.vector_store import query_vector_store
from rag_engine.llm import call_llm, count_tokens, suggest_followups
from rag_engine.reranker import rerank_results
import os


def main():
    print("=== Domain-AI Copilot (Phase 2) ===")
    while True:
        query = input("\nüîç Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n: ")
        if query.lower() in ["exit", "quit"]:
            break

        query_embedding = get_embedding(query)
        top_k_results = query_vector_store(query_embedding, top_k=10)

        # Re-rank c√°c k·∫øt qu·∫£
        reranked = rerank_results(query, top_k_results, top_n=5)

        print("\nüìÑ Top 5 t√†i li·ªáu ƒë∆∞·ª£c truy xu·∫•t (sau khi rerank):")
        context = ""
        for i, doc in enumerate(reranked):
            metadata = doc.payload or {}
            content = metadata.get("text", "<no content>")
            score = doc.score
            source = metadata.get("source", "unknown")
            chunk_id = metadata.get("chunk_id", "?")
            print(f"[{i+1}] (Score: {score:.4f}) - {source} [Chunk #{chunk_id}]\n{content}\n")
            context += content + "\n"

        token_count = count_tokens(context)
        print(f"üß† T·ªïng s·ªë tokens d√πng l√†m context: {token_count}")

        final_prompt = f"Tr·∫£ l·ªùi c√¢u h·ªèi sau d·ª±a v√†o th√¥ng tin sau:\n---\n{context}\n---\nC√¢u h·ªèi: {query}"
        answer = call_llm(final_prompt)

        print("\nü§ñ Tr·∫£ l·ªùi:")
        print(answer)

        suggestions = suggest_followups(answer)
        print("\nüí° G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo:")
        for i, sug in enumerate(suggestions, 1):
            print(f"{i}. {sug}")


if __name__ == "__main__":
    main()


===== ./llm_utils.py =====
# llm_utils.py
import os
import requests
from dotenv import load_dotenv

load_dotenv()
API_KEY = os.getenv("OPENROUTER_API_KEY")

def call_llm(prompt, model="mistralai/mistral-7b-instruct"):
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.5
    }

    response = requests.post(url, headers=headers, json=payload)
    result = response.json()
    return result["choices"][0]["message"]["content"]


===== ./app.py =====
# app.py - Streamlit UI for Domain-AI Copilot Phase 4

import streamlit as st
from rag_engine.embedding import get_embedding_cached
from rag_engine.vector_store import query_vector_store, init_qdrant
from rag_engine.llm import (
    call_llm_cached, count_tokens,
    suggest_followups, build_prompt_with_history,
    is_editing_previous_query
)
from rag_engine.reranker import rerank_with_cohere
from rag_engine.project_builder import create_project_from_prompt
from rag_engine.auto_builder import (
    create_and_run_project_from_prompt,
    debug_and_fix_project_forever
)

st.set_page_config(page_title="Domain-AI Copilot")
st.title("ü§ñ Domain-AI Copilot ‚Äì RAG Chatbot")

init_qdrant()

# Sidebar ‚Äì t·∫°o v√† ch·∫°y d·ª± √°n AI t·ª± ƒë·ªông
with st.sidebar:
    st.subheader("üöÄ T·∫°o d·ª± √°n AI m·ªõi")
    project_prompt = st.text_area("Nh·∫≠p y√™u c·∫ßu d·ª± √°n:", placeholder="VD: T·∫°o d·ª± √°n YOLOv8 ph√°t hi·ªán m≈© b·∫£o hi·ªÉm t·ª´ d·ªØ li·ªáu helmet_data/")
    
    if st.button("T·∫°o d·ª± √°n") and project_prompt:
        with st.spinner("üîß ƒêang t·∫°o d·ª± √°n t·ª´ AI..."):
            project_path = create_project_from_prompt(project_prompt)
            st.success(f"‚úÖ ƒê√£ t·∫°o d·ª± √°n t·∫°i `{project_path}`")

    if st.button("T·∫°o & Ch·∫°y d·ª± √°n AI") and project_prompt:
        with st.spinner("‚öôÔ∏è ƒêang x·ª≠ l√Ω v√† ch·∫°y th·ª≠ d·ª± √°n..."):
            result = create_and_run_project_from_prompt(project_prompt)
            st.success("‚úÖ ƒê√£ ch·∫°y xong d·ª± √°n!")
            st.code(result, language="bash")

    if st.button("üîß Debug & Fix T·ª± ƒë·ªông") and project_prompt:
        with st.spinner("‚ôªÔ∏è ƒêang debug cho ƒë·∫øn khi ch·∫°y th√†nh c√¥ng..."):
            debug_result = debug_and_fix_project_forever(project_prompt)
            st.success("‚úÖ Debug ho√†n t·∫•t, m√£ ƒë√£ ch·∫°y th√†nh c√¥ng!")

            # üì§ Hi·ªÉn th·ªã k·∫øt qu·∫£ run
            if debug_result.get("run_output"):
                st.subheader("üì§ K·∫øt qu·∫£ ch·∫°y `main.py`:")
                st.code(debug_result["run_output"], language="bash")

            # üìÑ Hi·ªÉn th·ªã c√°c file code ƒë√£ sinh/fix
            if debug_result.get("files"):
                for file_path, code in debug_result["files"].items():
                    st.subheader(f"üìÑ {file_path}")
                    st.code(code, language="python")

# Chat RAG
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

query = st.chat_input("H·ªèi t√†i li·ªáu k·ªπ thu·∫≠t...")

if query:
    previous_turns = st.session_state.chat_history
    is_edit = is_editing_previous_query(query)
    full_prompt = build_prompt_with_history(query, previous_turns) if not is_edit else query

    embedded_query = get_embedding_cached(query)
    docs = query_vector_store(embedded_query, top_k=10)
    reranked_docs = rerank_with_cohere(query, docs) if docs else []

    context = "\n\n".join([doc["text"] for doc in reranked_docs]) if reranked_docs else ""
    prompt = f"D·ª±a v√†o ng·ªØ c·∫£nh sau, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ng·∫Øn g·ªçn.\n\nNg·ªØ c·∫£nh:\n{context}\n\nC√¢u h·ªèi: {full_prompt}"

    answer = call_llm_cached(prompt)
    st.session_state.chat_history.append({"query": query, "answer": answer})

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for turn in st.session_state.chat_history:
    with st.chat_message("user"):
        st.markdown(turn["query"])
    with st.chat_message("ai"):
        st.markdown(turn["answer"])

# G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo
if st.session_state.chat_history:
    with st.expander("‚ú® G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo"):
        last_answer = st.session_state.chat_history[-1]["answer"]
        suggestions = suggest_followups(last_answer)
        for s in suggestions:
            st.markdown(f"- {s}")

# Tr√≠ch ngu·ªìn t√†i li·ªáu
def format_citation(doc, idx):
    source = doc.get("source", "unknown")
    chunk_id = doc.get("chunk_id", "?")
    return f"[{idx+1}] {source} (chunk #{chunk_id})"

if query and reranked_docs:
    st.caption("üìö Tr√≠ch ngu·ªìn:")
    for i, doc in enumerate(reranked_docs):
        st.markdown(f"- {format_citation(doc, i)}")


===== ./generated_projects/project_20250717_115520/main.py =====
import argparse
import torch
from ultralytics import YOLO
from utils.helper import load_config

def train(config):
    """Train YOLOv8 model on MNIST dataset."""
    model = YOLO(config['model_name'])
    model.train(data=config['data'], epochs=config['epochs'], imgsz=config['img_size'])

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='config.yaml', help='Path to config file')
    args = parser.parse_args()
    
    config = load_config(args.config)
    train(config)

===== ./generated_projects/project_20250717_115520/utils/helper.py =====
import yaml

def load_config(config_path):
    """Load YAML config file."""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

def save_config(config, config_path):
    """Save YAML config file."""
    with open(config_path, 'w') as f:
        yaml.dump(config, f)

===== ./rag_engine/__init__.py =====


===== ./rag_engine/file_parser.py =====
import fitz
import pandas as pd
import docx
import tiktoken

CHUNK_SIZE = 300
CHUNK_OVERLAP = 50

def parse_pdf(file):
    doc = fitz.open(stream=file.read(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def parse_csv(file):
    df = pd.read_csv(file)
    return df.to_string()

def parse_docx(file):
    doc = docx.Document(file)
    text = "\n".join([para.text for para in doc.paragraphs])
    return text

def extract_text(file, file_type):
    if file_type == "pdf":
        return parse_pdf(file)
    elif file_type == "csv":
        return parse_csv(file)
    elif file_type == "docx":
        return parse_docx(file)
    else:
        return "Unsupported file type"
    
def smart_chunk(text: str, max_tokens=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """Chia nh·ªè vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n th√¥ng minh d·ª±a theo ng·ªØ nghƒ©a, ng·∫Øt theo d√≤ng ho·∫∑c ƒëo·∫°n."""
    encoding = tiktoken.get_encoding("cl100k_base")
    lines = text.splitlines()
    
    chunks = []
    current_chunk = []
    current_tokens = 0

    for line in lines:
        line = line.strip()
        if not line:
            continue
        line_tokens = len(encoding.encode(line))
        if current_tokens + line_tokens > max_tokens:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
                # t·∫°o ph·∫ßn ch·ªìng l·∫∑p
                if overlap > 0 and len(chunks) > 0:
                    overlap_tokens = 0
                    overlap_chunk = []
                    for prev_line in reversed(current_chunk):
                        tokens = len(encoding.encode(prev_line))
                        if overlap_tokens + tokens > overlap:
                            break
                        overlap_chunk.insert(0, prev_line)
                        overlap_tokens += tokens
                    current_chunk = overlap_chunk.copy()
                    current_tokens = overlap_tokens
                else:
                    current_chunk = []
                    current_tokens = 0
        current_chunk.append(line)
        current_tokens += line_tokens

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

===== ./rag_engine/auto_builder.py =====
import os
import json
from datetime import datetime
from rag_engine.utils import (
    write_code_files,
    extract_json_string,
    read_all_code_files,  # ‚úÖ ƒê√£ b·ªï sung ƒë√∫ng
)
from rag_engine.runner import run_project
from rag_engine.llm import call_llm


def get_code_from_prompt(user_prompt: str) -> dict:
    full_prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n vi·∫øt ·ª©ng d·ª•ng Python.

H√£y t·∫°o m·ªôt project Python ho√†n ch·ªânh d·ª±a tr√™n y√™u c·∫ßu sau:
"{user_prompt}"

Y√™u c·∫ßu:
- Tr·∫£ v·ªÅ **d∆∞·ªõi d·∫°ng JSON**, kh√¥ng th√™m m√¥ t·∫£ n√†o kh√°c.
- M·ªói key l√† t√™n file (vd: main.py, utils.py)
- M·ªói value l√† n·ªôi dung file t∆∞∆°ng ·ª©ng.

V√≠ d·ª• ƒë·ªãnh d·∫°ng JSON h·ª£p l·ªá:
{{
  "main.py": "print('hello')",
  "utils.py": "def helper(): pass"
}}

H√£y ch·∫Øc ch·∫Øn tr·∫£ v·ªÅ ƒë√∫ng JSON!
"""

    response = call_llm(full_prompt)
    print("===> Raw LLM response:\n", response)

    # Try to extract JSON
    try:
        file_dict = extract_json_string(response)
    except Exception as e:
        print("[WARNING] Kh√¥ng parse ƒë∆∞·ª£c JSON, fallback t·∫°o main.py")
        file_dict = {"main.py": response}

    return file_dict


def create_project_from_prompt(prompt: str) -> str:
    response = get_code_from_prompt(prompt)

    try:
        json_str = extract_json_string(response)
        files = json.loads(json_str)
    except Exception as e:
        raise ValueError(f"Output kh√¥ng ƒë√∫ng JSON:\n{response}") from e

    now = datetime.now().strftime("%Y%m%d_%H%M%S")
    project_path = os.path.join("generated_projects", f"project_{now}")
    os.makedirs(project_path, exist_ok=True)

    write_code_files(files, project_path)
    return project_path


def create_and_run_project_from_prompt(prompt: str) -> str:
    project_path = create_project_from_prompt(prompt)
    output = run_project(project_path)
    return output


def debug_and_fix_project_forever(prompt: str, max_rounds: int = 5) -> str:
    project_path = create_project_from_prompt(prompt)

    for round in range(max_rounds):
        print(f"\nüîÅ Debug Round {round + 1}")

        output = run_project(project_path)
        if "[ERROR]" not in output:
            print("‚úÖ Ch·∫°y th√†nh c√¥ng")
            return output

        # ƒê·ªçc to√†n b·ªô m√£ hi·ªán t·∫°i
        current_code = read_all_code_files(project_path)

        fix_prompt = f"""
    D∆∞·ªõi ƒë√¢y l√† m√£ ngu·ªìn c·ªßa m·ªôt project Python v√† l·ªói g·∫∑p ph·∫£i khi ch·∫°y. H√£y s·ª≠a l·ªói v√† tr·∫£ v·ªÅ l·∫°i t·∫•t c·∫£ c√°c file d∆∞·ªõi d·∫°ng JSON.

    ## Code hi·ªán t·∫°i:
    {current_code}
    ## L·ªói khi ch·∫°y:
    {output}
    Tr·∫£ v·ªÅ JSON m·ªõi cho to√†n b·ªô c√°c file ƒë√£ ch·ªânh s·ª≠a.
    """
    response = call_llm(fix_prompt)

    try:
        json_str = extract_json_string(response)
        new_code_files = json.loads(json_str)
    except Exception as e:
        raise ValueError(f"[‚ùå] JSON response kh√¥ng h·ª£p l·ªá:\n{response}") from e

    write_code_files(new_code_files, project_path)

    return f"[‚ÄºÔ∏è] ƒê√£ th·ª≠ {max_rounds} l·∫ßn nh∆∞ng v·∫´n l·ªói:\n{output}"

===== ./rag_engine/vector_store.py =====
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from qdrant_client.http.models import PointStruct, SearchParams
from typing import List, Tuple
import uuid
from dotenv import load_dotenv
import os

load_dotenv()

QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
QDRANT_HOST = os.getenv("QDRANT_HOST")
COLLECTION_NAME = os.getenv("COLLECTION_NAME")

client = QdrantClient(
    url=QDRANT_HOST,
    api_key=QDRANT_API_KEY
)

def init_qdrant(vector_size=1536):
    try:
        client.get_collection(COLLECTION_NAME)
        print(f"Collection '{COLLECTION_NAME}' already exists.")
    except Exception:
        print(f"Creating collection '{COLLECTION_NAME}' with vector size {vector_size}...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(
                size=vector_size,
                distance=Distance.COSINE
            )
        )

def add_embeddings_to_qdrant(texts: List[str], embeddings: List[List[float]]):
    """
    L∆∞u text v√† embedding t∆∞∆°ng ·ª©ng v√†o Qdrant
    """
    expected_dim = int(os.getenv("EMBEDDING_DIM"))
    actual_dim = len(embeddings[0])
    if actual_dim != expected_dim:
        raise ValueError(f"Mismatch embedding dimension: expected {expected_dim}, got {actual_dim}")

    points = [
        PointStruct(
            id=str(uuid.uuid4()),
            vector=embedding,
            payload={"text": text},
        )
        for text, embedding in zip(texts, embeddings)
    ]
    client.upsert(collection_name=COLLECTION_NAME, points=points)

def search_similar(query_embedding: List[float], top_k: int = 3) -> List[Tuple[str, float]]:
    search_result = client.search(
        collection_name=COLLECTION_NAME,
        query_vector=query_embedding,
        limit=top_k,
        with_payload=True,
    )
    results = []
    for item in search_result:
        text = item.payload.get("text", "")
        score = item.score
        results.append((text, score))
    return results

def query_vector_store(embedding: list[float], top_k: int = 5):
    hits = client.search(
        collection_name=COLLECTION_NAME,
        query_vector=embedding,
        limit=top_k,
        search_params=SearchParams(hnsw_ef=128),
        with_payload=True,
        with_vectors=False
    )
    return hits

===== ./rag_engine/project_builder.py =====
# rag_engine/project_builder.py

import os
import hashlib

from rag_engine.llm import call_llm_cached

def slugify(text):
    """T·∫°o t√™n th∆∞ m·ª•c ng·∫Øn g·ªçn t·ª´ prompt"""
    hash_object = hashlib.md5(text.encode())
    return hash_object.hexdigest()[:8]

def create_project_from_prompt(prompt: str) -> str:
    """T·∫°o project Python t·ª´ prompt v√† l∆∞u v√†o th∆∞ m·ª•c m·ªõi"""
    folder_name = slugify(prompt)
    project_path = os.path.join("generated_projects", folder_name)
    os.makedirs(project_path, exist_ok=True)

    code_prompt = f"""Vi·∫øt m·ªôt file `main.py` th·ª±c hi·ªán y√™u c·∫ßu sau:\n\n{prompt}\n\nH√£y tr·∫£ v·ªÅ to√†n b·ªô n·ªôi dung file."""
    code = call_llm_cached(code_prompt)

    main_path = os.path.join(project_path, "main.py")
    with open(main_path, "w", encoding="utf-8") as f:
        f.write(code)

    return project_path


===== ./rag_engine/utils.py =====
import json
import os
import re

def extract_json_string(text: str) -> str:
    """
    Tr√≠ch xu·∫•t ƒëo·∫°n JSON t·ª´ text c√≥ th·ªÉ ch·ª©a c·∫£ markdown code block.
    """
    try:
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            text = text.split("```")[1].strip()
        return text
    except Exception as e:
        raise ValueError("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t JSON t·ª´ text") from e

def write_code_files(file_dict: dict, base_path: str):
    """
    Ghi c√°c file code t·ª´ dict JSON xu·ªëng th∆∞ m·ª•c base_path.
    """
    os.makedirs(base_path, exist_ok=True)
    for file_path, code in file_dict.items():
        full_path = os.path.join(base_path, file_path)
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        with open(full_path, "w", encoding="utf-8") as f:
            f.write(code)

def read_all_code_files(base_path: str) -> dict:
    """
    ƒê·ªçc to√†n b·ªô m√£ ngu·ªìn t·ª´ th∆∞ m·ª•c base_path, tr·∫£ v·ªÅ dict d·∫°ng {filepath: code}.
    """
    file_dict = {}
    for root, _, files in os.walk(base_path):
        for file in files:
            path = os.path.join(root, file)
            rel_path = os.path.relpath(path, base_path)
            with open(path, "r", encoding="utf-8") as f:
                file_dict[rel_path] = f.read()
    return file_dict

def extract_json_string(text: str) -> dict:
    """
    T√¨m ƒëo·∫°n JSON trong text (n·∫øu b·ªã b·ªçc trong markdown ```json ... ``` th√¨ c·∫Øt ra)
    """
    json_match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL)
    if json_match:
        json_str = json_match.group(1)
    else:
        json_str = text.strip()

    return json.loads(json_str)

===== ./rag_engine/reranker.py =====
# rag_engine/reranker.py

import os
import cohere
from dotenv import load_dotenv

load_dotenv()
COHERE_API_KEY = os.getenv("COHERE_API_KEY")

co = cohere.Client(COHERE_API_KEY)

def rerank_with_cohere(query: str, documents: list[str], top_n: int = 5):
    response = co.rerank(
        model="rerank-v3.5",
        query=query,
        documents=documents,
        top_n=top_n,
    )
    reranked = [
        {"text": documents[r.index], "score": r.relevance_score}
        for r in response.results
    ]
    return reranked


===== ./rag_engine/llm.py =====
import os
from openai import OpenAI
from dotenv import load_dotenv
import tiktoken
import re
import hashlib
import pickle

load_dotenv()

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY"),
)

CACHE_DIR = ".cache/llm"
os.makedirs(CACHE_DIR, exist_ok=True)

LLM_MODEL = os.getenv("LLM_MODEL", "deepseek/deepseek-chat-v3-0324:free")

def call_llm(prompt: str) -> str:
    response = client.chat.completions.create(
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªó tr·ª£ t√¨m ki·∫øm v√† tr·∫£ l·ªùi k·ªπ thu·∫≠t t·ª´ t√†i li·ªáu."},
            {"role": "user", "content": prompt}
        ],
    )
    return response.choices[0].message.content

def count_tokens(text: str) -> int:
    # D√πng tiktoken ƒë·ªÉ ƒë·∫øm token (s·ª≠ d·ª•ng encoding g·∫ßn ƒë√∫ng theo gpt-3.5)
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(encoding.encode(text))

def suggest_followups(answer: str) -> list[str]:
    prompt = f"D·ª±a v√†o c√¢u tr·∫£ l·ªùi sau, h√£y g·ª£i √Ω 3 c√¢u h·ªèi ti·∫øp theo m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ h·ªèi:\n\n{answer}"
    result = call_llm(prompt)
    # Parse g·ª£i √Ω d·∫°ng g·∫°ch ƒë·∫ßu d√≤ng
    suggestions = [line.strip("-‚Ä¢ ").strip() for line in result.strip().split("\n") if line.strip()]
    return suggestions[:3]

def build_prompt_with_history(current_query: str, chat_history: list[dict], max_turns: int = 3) -> str:
    selected = chat_history[-max_turns:] if len(chat_history) >= max_turns else chat_history
    prompt = "D∆∞·ªõi ƒë√¢y l√† l·ªãch s·ª≠ h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥:\n"
    for i, h in enumerate(selected):
        prompt += f"\n[L∆∞·ª£t {i+1}]\nQ: {h['query']}\nA: {h['answer']}\n"
    prompt += f"\n[L∆∞·ª£t hi·ªán t·∫°i]\nQ: {current_query}"
    return prompt

def is_editing_previous_query(query: str) -> bool:
    return bool(re.search(r"s·ª≠a c√¢u\\s*\\d+", query.lower()) or "n√≥i l·∫°i" in query.lower())

def _hash_prompt(prompt: str) -> str:
    return hashlib.sha256(prompt.encode("utf-8")).hexdigest()

def _cache_path(prompt: str) -> str:
    return os.path.join(CACHE_DIR, _hash_prompt(prompt) + ".pkl")


def call_llm(prompt: str, system_prompt: str = "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI.", model="deepseek/deepseek-chat-v3-0324:free") -> str:
    completion = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt},
        ],
    )
    return completion.choices[0].message.content.strip()

def call_llm_cached(prompt: str, model="deepseek/deepseek-chat-v3-0324:free") -> str:
    path = _cache_path(prompt)
    if os.path.exists(path):
        with open(path, "rb") as f:
            return pickle.load(f)
    result = call_llm(prompt, model)
    with open(path, "wb") as f:
        pickle.dump(result, f)
    return result

===== ./rag_engine/embedding.py =====
import os
from openai import OpenAI
from dotenv import load_dotenv
import hashlib
import pickle

load_dotenv()

COHERE_API_KEY = os.getenv("COHERE_API_KEY")
MODEL_EMBEDDING = os.getenv("MODEL_EMBEDDING")

CACHE_DIR = ".cache/embeddings"
os.makedirs(CACHE_DIR, exist_ok=True)

client = OpenAI(
    base_url="https://api.cohere.ai/compatibility/v1",
    api_key=COHERE_API_KEY,
)

def embed_texts(texts: list[str]) -> list[list[float]]:
    response = client.embeddings.create(
        input=texts,
        model=MODEL_EMBEDDING,
        encoding_format="float",
    )
    return [d.embedding for d in response.data]

def _hash_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def _cache_path(text: str) -> str:
    return os.path.join(CACHE_DIR, _hash_text(text) + ".pkl")

def get_embedding(text: str, model=MODEL_EMBEDDING) -> list[float]:
    response = client.embeddings.create(
        input=[text],
        model=model,
    )
    return response.data[0].embedding

def get_embedding_cached(text: str, model=MODEL_EMBEDDING) -> list[float]:
    path = _cache_path(text)
    if os.path.exists(path):
        with open(path, "rb") as f:
            return pickle.load(f)
    embedding = get_embedding(text, model)
    with open(path, "wb") as f:
        pickle.dump(embedding, f)
    return embedding

===== ./rag_engine/runner.py =====
import os
import subprocess

def run_project(project_path: str) -> str:
    """
    Ch·∫°y file main.py trong th∆∞ m·ª•c project ƒë√£ t·∫°o.
    Tr·∫£ v·ªÅ stdout ho·∫∑c th√¥ng b√°o l·ªói n·∫øu c√≥.
    """
    main_path = os.path.join(project_path, "main.py")
    
    if not os.path.exists(main_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file main.py trong: {project_path}")
    
    try:
        # Ch·∫°y file main.py v√† capture output
        result = subprocess.run(
            ["python", main_path],
            cwd=project_path,
            capture_output=True,
            text=True,
            timeout=30  # tr√°nh infinite loop
        )
        
        if result.returncode != 0:
            return f"[ERROR] {result.stderr}"
        
        return result.stdout
    
    except subprocess.TimeoutExpired:
        return "[ERROR] Qu√° th·ªùi gian ch·∫°y (timeout)."
    except Exception as e:
        return f"[ERROR] L·ªói khi ch·∫°y project: {str(e)}"


===== DIRECTORY TREE =====
./
    test.py
    llm_utils.py
    LICENSE
    app.py
    .env
    generated_projects/
        project_20250717_115520/
            main.py
            config.yaml
            utils/
                helper.py
    rag_engine/
        __init__.py
        file_parser.py
        auto_builder.py
        vector_store.py
        project_builder.py
        utils.py
        reranker.py
        llm.py
        embedding.py
        runner.py
    _utils/
    .cache/
        embeddings/
            234fff5d726d205d7273b4f2667dc765c1119556b5a999fd73a400f1102dfb5e.pkl
        llm/
            3871c5b0c2965113343d18dbbbbba93a40a913c8a33ffbd594cc7b1f26cceb8c.pkl
            b1a575625c502b9022237303c33562a477260ba29c3a12f5b4c1f4eb43888e3c.pkl
            bdc9ec7e9091fc8819c466ee88a46bfa4828dce5544462e950b788a575a2ba4e.pkl
            e210cfafa497edd70d0c0348fde4adfdd654475b1172332b5319fc9fd48fde0e.pkl
            3ab955c4397e2b8304cbe7de455fca6ab333ddb8755fffb5f2b0291cc5bb8527.pkl
            957b0d9d7bfd65fa8e7eeff1b167239495e05aaffc6d379e4b2cb9c65f6986d0.pkl
            5d2146133abbf9b0e861c5626eb7d2ce78af40c95bb80dd63b5498c8cdfd4877.pkl
            e9acf42b0817a6d9c2949493577e81addf45eb9a6bedfdba5990a3c73bab6a76.pkl
