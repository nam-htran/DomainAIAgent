# app.py - Streamlit UI for Domain-AI Copilot (Final Version)

import streamlit as st
from rag_engine.embedding import get_embedding_cached
from rag_engine.vector_store import query_vector_store, init_qdrant
from rag_engine.llm import call_llm_cached, suggest_followups, create_standalone_query_from_history
from rag_engine.reranker import rerank_with_cohere
from rag_engine.data_loader import process_and_load_files

# ----- C·∫•u h√¨nh trang v√† ti√™u ƒë·ªÅ -----
st.set_page_config(
    page_title="AI Copilot - RAG Chatbot",
    page_icon="ü§ñ",
    layout="wide"
)
st.title("ü§ñ AI Copilot ‚Äì Chatbot H·ªèi-ƒê√°p Th√¥ng Minh")

# ----- Kh·ªüi t·∫°o c∆° s·ªü d·ªØ li·ªáu vector -----
init_qdrant()

# ----- Sidebar ƒë·ªÉ t·∫£i v√† x·ª≠ l√Ω t√†i li·ªáu -----
with st.sidebar:
    st.header("üìö Qu·∫£n l√Ω C∆° s·ªü Tri th·ª©c")
    uploaded_files = st.file_uploader(
        "T·∫£i l√™n c√°c file PDF, DOCX, ho·∫∑c TXT",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True,
    )
    if st.button("X·ª≠ l√Ω v√† N·∫°p v√†o CSDL", disabled=not uploaded_files, use_container_width=True):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            stats = process_and_load_files(uploaded_files)
            st.success(f"Ho√†n t·∫•t! ƒê√£ th√™m {stats['new_chunks_added']} ƒëo·∫°n n·ªôi dung m·ªõi.")

# ----- Qu·∫£n l√Ω v√† Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat -----
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
for turn in st.session_state.chat_history:
    with st.chat_message(turn["role"]):
        st.markdown(turn["content"])

# ----- X·ª≠ l√Ω khi ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi m·ªõi -----
if query := st.chat_input("H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨..."):
    st.chat_message("user").markdown(query)
    st.session_state.chat_history.append({"role": "user", "content": query})

    with st.spinner("üß† AI ƒëang suy nghƒ©..."):
        # ======================================================================
        # B∆Ø·ªöC 0: VI·∫æT L·∫†I C√ÇU H·ªéI D·ª∞A TR√äN L·ªäCH S·ª¨ (HISTORY-AWARE)
        # ======================================================================
        if len(st.session_state.chat_history) > 1:
            standalone_query = create_standalone_query_from_history(st.session_state.chat_history, query)
            # Hi·ªÉn th·ªã c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c di·ªÖn gi·∫£i ƒë·ªÉ ng∆∞·ªùi d√πng bi·∫øt
            if standalone_query.lower() != query.lower():
                st.info(f"ƒê√£ di·ªÖn gi·∫£i c√¢u h·ªèi th√†nh: *\"{standalone_query}\"*")
        else:
            standalone_query = query

        # B∆Ø·ªöC 1: EMBEDDING C√ÇU H·ªéI ƒê·ªòC L·∫¨P
        query_embedding = query_embedding = get_embedding_cached(standalone_query)

        # B∆Ø·ªöC 2 & 3: TRUY XU·∫§T V√Ä S·∫ÆP X·∫æP L·∫†I
        retrieved_docs = query_vector_store(query_embedding, top_k=10)
        reranked_docs = rerank_with_cohere(query=standalone_query, documents=retrieved_docs, top_n=5) if retrieved_docs else []

        # ======================================================================
        # B∆Ø·ªöC 4 & 5: T·∫†O PROMPT V√Ä SINH C√ÇU TR·∫¢ L·ªúI (C√ì LOGIC FALLBACK)
        # ======================================================================
        if not reranked_docs:
            # ------- K·ªäCH B·∫¢N 1: KH√îNG T√åM TH·∫§Y T√ÄI LI·ªÜU (FALLBACK TO LLM) -------
            st.warning("Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu. Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung.")
            system_prompt = "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch tr·ª±c ti·∫øp."
            final_prompt = standalone_query
            answer = call_llm_cached(final_prompt, system_prompt)
            
            st.chat_message("assistant").markdown(answer)
            st.session_state.chat_history.append({"role": "assistant", "content": answer})
        else:
            # ------- K·ªäCH B·∫¢N 2: T√åM TH·∫§Y T√ÄI LI·ªÜU (RAG) -------
            context = "\n\n---\n\n".join([doc.payload.get("text", "") for doc in reranked_docs])
            system_prompt = "D·ª±a v√†o ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c v√† chi ti·∫øt. N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu ƒë√£ cung c·∫•p."
            final_prompt = f"**Ng·ªØ c·∫£nh:**\n{context}\n\n**C√¢u h·ªèi:**\n{standalone_query}"
            
            answer = call_llm_cached(final_prompt, system_prompt)
            st.chat_message("assistant").markdown(answer)
            st.session_state.chat_history.append({"role": "assistant", "content": answer})

            # Hi·ªÉn th·ªã ngu·ªìn tr√≠ch d·∫´n
            with st.expander("üìö Xem ngu·ªìn tr√≠ch d·∫´n"):
                for i, doc in enumerate(reranked_docs):
                    metadata = doc.payload or {}
                    source = metadata.get("source", "Kh√¥ng r√µ ngu·ªìn")
                    st.markdown(f"**[{i+1}] Ngu·ªìn:** `{source}`")
                    st.caption(f"```{metadata.get('text', '')[:200]}...```")

        # G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo
        with st.expander("üí° G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo"):
            suggestions = suggest_followups(answer)
            for sug in suggestions:
                st.markdown(f"- {sug}")